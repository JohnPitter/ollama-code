# ü§ñ Ollama Code - AI Code Assistant

> Assistente de c√≥digo AI inteligente que funciona como Claude Code, 100% local, escrito em Go.

[![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?style=flat&logo=go)](https://go.dev/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Platform](https://img.shields.io/badge/Platform-Linux%20%7C%20Windows%20%7C%20macOS-lightgrey)]()

---

## ‚ú® Caracter√≠sticas

### Base Features
- üß† **Linguagem Natural** - Sem comandos especiais (`/read`, `/exec`), apenas fale naturalmente
- üéØ **Detec√ß√£o Inteligente** - IA detecta automaticamente suas inten√ß√µes usando LLM
- üîß **8+ Ferramentas** - Leitura/escrita de arquivos, execu√ß√£o de comandos, git, an√°lise de c√≥digo
- üì∑ **Suporte a Imagens** - L√™ e analisa imagens (PNG, JPG, GIF, etc)
- üåê **Pesquisa Web** - Busca na internet quando necess√°rio (DuckDuckGo, Stack Overflow, GitHub)
- üéõÔ∏è **3 Modos de Opera√ß√£o**:
  - **READ-ONLY**: Somente leitura
  - **INTERACTIVE**: Com confirma√ß√£o (padr√£o)
  - **AUTONOMOUS**: Totalmente autom√°tico
- ‚ö° **Performance M√°xima** - Startup <15ms, streaming em tempo real
- üîí **Privacidade** - 100% local, sem envio de dados para nuvem

### Enterprise Features ‚ú® NEW!
- üíæ **Checkpoints & Recovery** - Volte no tempo, desfa√ßa mudan√ßas, recupere estados anteriores
- üìÇ **Session Management** - Salve e retome sess√µes de trabalho
- üß† **Hierarchical Memory** - 5 n√≠veis de mem√≥ria (Enterprise ‚Üí Project ‚Üí Rules ‚Üí User ‚Üí Local)
- ‚ö° **Slash Commands** - 10+ comandos built-in (/help, /checkpoint, /session, /doctor, etc)
- ü™ù **Hooks System** - Pre/post hooks para valida√ß√£o e automa√ß√£o
- üé® **Output Styles** - 4 estilos de output (default, explanatory, learning, corporate)
- üöÄ **Performance** - Context cache, async tasks, otimiza√ß√µes
- üè• **Diagnostics** - /doctor para health checks completos
- üñ•Ô∏è **Hardware Auto-Detection** - Detecta seu hardware e otimiza automaticamente
- ‚öôÔ∏è **3 Presets de Configura√ß√£o** - Compatibility, Performance e Ultra

---

## üéØ Objetivo

Criar um assistente de c√≥digo que funciona como Claude Code, mas rodando completamente local usando Ollama.

**Exemplo de uso:**
```bash
$ ollama-code chat

Voc√™: Cria um servidor HTTP em Go com endpoint /health

ü§ñ: Vou criar um servidor HTTP b√°sico...

üîî Confirma√ß√£o necess√°ria:
   A√ß√£o: Criar arquivo server.go
   Tipo: WRITE_FILE

Executar? [y/N]: y

‚úÖ Arquivo criado: server.go

ü§ñ: Servidor criado! Quer que eu execute para testar?
```

---

## üìã Requisitos

### Hardware Alvo
- **CPU**: Intel i9 14¬™ gen (24 cores) ou similar
- **RAM**: 64GB
- **GPU**: NVIDIA RTX Ada 2000 (16GB VRAM) ou similar
- **Storage**: 1TB NVMe SSD

### Software
- **Go**: 1.21+
- **Ollama**: √öltima vers√£o
- **CUDA**: 11.8+ (para GPU NVIDIA)
- **OS**: Linux, Windows ou macOS

---

## üöÄ Instala√ß√£o R√°pida

### 1. Instalar depend√™ncias

**Go:**
```bash
# Linux
wget https://go.dev/dl/go1.21.6.linux-amd64.tar.gz
sudo tar -C /usr/local -xzf go1.21.6.linux-amd64.tar.gz
echo 'export PATH=$PATH:/usr/local/go/bin' >> ~/.bashrc
source ~/.bashrc

# Windows: Baixar de https://go.dev/dl/
# macOS: brew install go@1.21
```

**Ollama:**
```bash
# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows: Baixar de https://ollama.ai/download/windows
# macOS: brew install ollama
```

### 2. Configurar GPU

**Linux/macOS** (`~/.config/ollama/env.conf`):
```bash
export OLLAMA_GPU_LAYERS=999
export OLLAMA_NUM_GPU=1
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_FLASH_ATTENTION=1
export OLLAMA_MAX_VRAM=16384
```

**Windows** (PowerShell como Admin):
```powershell
[System.Environment]::SetEnvironmentVariable('OLLAMA_GPU_LAYERS', '999', 'Machine')
[System.Environment]::SetEnvironmentVariable('OLLAMA_NUM_GPU', '1', 'Machine')
Restart-Service Ollama
```

### 3. Baixar modelo

```bash
ollama pull qwen2.5-coder:32b-instruct-q6_K
```

**Ambiente corporativo com proxy?** Use os scripts de download direto:
```bash
# Linux/macOS
chmod +x download-models-direct.sh
./download-models-direct.sh

# Windows
.\download-models-direct.ps1
```

### 4. Build e instalar

```bash
# Clone/baixe o projeto
cd ollama-code

# Build
make build
# ou (se n√£o tiver make):
./build.sh          # Linux/macOS
.\build.bat         # Windows

# Instalar globalmente
make install
```

Pronto! Agora voc√™ pode usar:
```bash
ollama-code chat
```

### 5. Primeira Execu√ß√£o - Hardware Auto-Detection üÜï

Na primeira vez que voc√™ executar o Ollama Code, ele ir√°:

1. **Detectar automaticamente seu hardware**:
   - CPU (modelo, cores, threads)
   - RAM (total e dispon√≠vel)
   - GPU NVIDIA (modelo, VRAM, quantidade)
   - Espa√ßo em disco
   - Sistema operacional

2. **Classificar sua m√°quina** em um tier de performance:
   - **High-end**: 32GB+ RAM, GPU 16GB+, 8+ cores
   - **Mid-range**: 16GB+ RAM, GPU 8GB+, 4+ cores
   - **Entry**: 8GB+ RAM
   - **Low-end**: < 8GB RAM

3. **Aplicar automaticamente o melhor preset**:
   - **Ultra**: Para high-end (modelo 32B, todas as otimiza√ß√µes)
   - **Performance**: Para mid-range (modelo 14B-32B, balanceado)
   - **Compatibility**: Para entry/low-end (modelo 7B, m√≠nimo de recursos)

4. **Gerar e salvar** `~/.ollama-code/config.json` otimizado

```bash
$ ollama-code chat

üîç First run detected - Analyzing your hardware...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë          OLLAMA CODE - HARDWARE DETECTION REPORT           ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üñ•Ô∏è  HARDWARE DETECTED:
   CPU: Intel(R) Core(TM) i9-14900K
   Cores/Threads: 24 / 32
   RAM: 65536 MB total (52428 MB available)
   GPU: NVIDIA RTX Ada 2000
   VRAM: 16384 MB (1 GPU(s))
   Disk Space: 512 GB available
   OS: windows / amd64

‚ö° PERFORMANCE TIER: high-end

üéØ PRESET SELECTED: ultra
   Ultra - M√°xima performance, requer hardware potente (modelo 32B)

‚öôÔ∏è  OPTIMIZED CONFIGURATION:
   Model: qwen2.5-coder:32b-instruct-q6_K
   GPU Layers: 999 (all)
   Max VRAM: 15564 MB (95% of available)
   Parallel Requests: 6
   Flash Attention: true
   Checkpoints: true (retention: 30 days, max: 100)

‚úÖ Configuration optimized for your hardware!
   Config saved to: ~/.ollama-code/config.json
```

**Quer customizar?** Edite `~/.ollama-code/config.json` ou veja [CONFIG.md](CONFIG.md)

---

## üìñ Uso

### Modo Interativo (Padr√£o)

```bash
ollama-code chat

Voc√™: Analisa esse projeto
ü§ñ: [L√™ arquivos e explica a estrutura]

Voc√™: Cria um servidor REST em Go
ü§ñ: [Gera c√≥digo e pede confirma√ß√£o antes de criar arquivo]
```

### Modo Read-Only (Somente Leitura)

```bash
ollama-code chat --mode readonly

Voc√™: Mostra o main.go
ü§ñ: [Mostra conte√∫do]

Voc√™: Corrija os erros
‚ùå A√ß√£o bloqueada: Escrita n√£o permitida em modo READ-ONLY
```

### Modo Aut√¥nomo (Sem Confirma√ß√£o)

```bash
ollama-code chat --mode autonomous

Voc√™: Cria um projeto completo com CRUD e testes

[10:23:45] ‚úì Criado: main.go
[10:23:46] ‚úì Criado: handlers/user.go
[10:23:47] ‚úì Criado: tests/user_test.go
[10:23:48] ‚öôÔ∏è  go mod tidy
[10:23:49] ‚öôÔ∏è  go test ./...
[10:23:52] ‚úÖ Testes passando
```

### Leitura de Imagens

```bash
Voc√™: Leia a imagem screenshot.png e me diga o que tem nela

ü§ñ: [L√™ e analisa a imagem]
    A imagem mostra uma interface de usu√°rio com...
```

### Pesquisa na Internet

```bash
Voc√™: Como corrigir erro "permission denied" no Docker?

üåê Pesquisando na internet...
‚úì Encontrei 3 fontes relevantes

ü§ñ: O erro ocorre quando... [solu√ß√£o com exemplos]

üìö Fontes:
[1] Stack Overflow - https://...
[2] Docker Docs - https://...
```

---

## üéõÔ∏è Modos de Opera√ß√£o

| Modo | Flag | Descri√ß√£o | Uso Recomendado |
|------|------|-----------|-----------------|
| **INTERACTIVE** | `--mode interactive` (padr√£o) | Confirma a√ß√µes destrutivas | Desenvolvimento do dia a dia |
| **READ-ONLY** | `--mode readonly` | Apenas leitura | Code review, explora√ß√£o |
| **AUTONOMOUS** | `--mode autonomous` | Tudo autom√°tico | Automa√ß√£o, prototipagem |

---

## üîß Ferramentas Dispon√≠veis

O sistema detecta automaticamente qual ferramenta usar:

- **FileReader**: L√™ arquivos de texto e imagens
- **FileWriter**: Escreve/modifica arquivos
- **CommandExecutor**: Executa comandos shell
- **CodeSearcher**: Busca em c√≥digo (ripgrep)
- **ProjectAnalyzer**: Analisa estrutura do projeto
- **GitOperations**: Opera√ß√µes git (commit, push, etc)
- **WebSearcher**: Pesquisa na internet

**Voc√™ n√£o precisa especificar qual ferramenta usar** - a IA escolhe automaticamente baseado no seu pedido!

---

## üìä Performance

**No hardware alvo (i9 14¬™ gen + RTX Ada 2000):**

```
Startup time:      < 15ms
Memory (base):     ~10MB
Binary size:       ~8MB (otimizado)
LLM throughput:    ~30-40 tokens/s
File operations:   < 10ms
Web search:        ~2-5s (cache: <100ms)
```

---

## üõ†Ô∏è Desenvolvimento

### Build

```bash
# Build padr√£o
make build

# Build otimizado (produ√ß√£o)
make build-optimized

# Executar sem instalar
make run

# Testes
make test

# Limpar
make clean
```

### Estrutura do Projeto

```
ollama-code/
‚îú‚îÄ‚îÄ cmd/ollama-code/main.go          # Entry point
‚îú‚îÄ‚îÄ internal/
‚îÇ   ‚îú‚îÄ‚îÄ agent/                       # Agente principal
‚îÇ   ‚îú‚îÄ‚îÄ intent/                      # Detec√ß√£o de inten√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ tools/                       # Ferramentas
‚îÇ   ‚îú‚îÄ‚îÄ websearch/                   # Pesquisa web
‚îÇ   ‚îú‚îÄ‚îÄ llm/                         # Client Ollama
‚îÇ   ‚îî‚îÄ‚îÄ confirmation/                # Confirma√ß√µes
‚îú‚îÄ‚îÄ Makefile
‚îî‚îÄ‚îÄ IMPLEMENTATION_PLAN.md           # Plano completo
```

---

## üìö Documenta√ß√£o

- **[IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md)** - Plano t√©cnico completo de implementa√ß√£o (base)
- **[ENTERPRISE_FEATURES.md](ENTERPRISE_FEATURES.md)** - Funcionalidades enterprise-grade completas
- **[download-models-direct.sh](download-models-direct.sh)** - Script para download de modelos (Linux/macOS)
- **[download-models-direct.ps1](download-models-direct.ps1)** - Script para download de modelos (Windows)

---

## ü§ù Contribuindo

Este projeto foi criado como um plano de implementa√ß√£o completo para ser executado por uma IA (como Grok Code Fast 1).

Para contribuir:
1. Leia o [IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md)
2. Siga a estrutura definida
3. Implemente fase por fase
4. Teste cada componente
5. Submeta PR

---

## üìù Exemplos Avan√ßados

### Criar projeto completo
```bash
Voc√™: Cria um projeto REST API em Go com:
      - CRUD de usu√°rios
      - Autentica√ß√£o JWT
      - Testes unit√°rios
      - Dockerfile
      - README

ü§ñ: [Cria estrutura completa do projeto]
```

### An√°lise e refatora√ß√£o
```bash
Voc√™: Analisa o c√≥digo e refatora seguindo Clean Code

ü§ñ: [Analisa, sugere melhorias e aplica refatora√ß√µes]
```

### Debug com pesquisa
```bash
Voc√™: Estou tendo erro X no c√≥digo, pesquise solu√ß√µes e corrija

ü§ñ: üåê Pesquisando...
    [Encontra solu√ß√£o, aplica corre√ß√£o]
```

---

## ‚ö†Ô∏è Avisos Importantes

1. **Modo Aut√¥nomo**: Use com cuidado! Todas as a√ß√µes s√£o executadas sem confirma√ß√£o.
2. **GPU**: Para melhor performance, configure todas as layers para rodar na GPU.
3. **Proxy Corporativo**: Use os scripts de download direto para baixar modelos.
4. **Backup**: Sempre fa√ßa backup antes de usar modo aut√¥nomo.

---

## üéØ Roadmap

### Base (10-12 dias)
- [x] Detec√ß√£o inteligente de inten√ß√µes
- [x] 3 modos de opera√ß√£o
- [x] Pesquisa na internet
- [x] Suporte a imagens
- [x] Streaming de respostas
- [x] 8+ ferramentas integradas

### Enterprise (24 dias adicionais)
- [x] **Checkpoints & Rewind** - Recupera√ß√£o de estado
- [x] **Session Management** - M√∫ltiplas sess√µes, resumir
- [x] **Hierarchical Memory** - CLAUDE.md em 5 n√≠veis
- [x] **40+ Slash Commands** - Customiz√°veis
- [x] **Hooks System** - Pre/Post execution
- [x] **Telemetry** - OpenTelemetry, m√©tricas
- [x] **Sandboxing** - Isolamento de processos
- [x] **/doctor** - Health checks & diagnostics
- [x] **Background Tasks** - Async execution
- [x] **CI/CD** - GitHub Actions, GitLab

### Futuro
- [ ] Cache de embeddings (Redis)
- [ ] Suporte a plugins MCP
- [ ] Interface web
- [ ] Integra√ß√£o com VS Code

---

## üìÑ Licen√ßa

Apache 2.0 - Veja [LICENSE](LICENSE)

---

## üë§ Autor

Criado como especifica√ß√£o t√©cnica completa para implementa√ß√£o por IA.

**Hardware alvo:** PC high-end com i9 14¬™ gen, 64GB RAM, RTX Ada 2000

---

## üôè Agradecimentos

- **Ollama** - Por fornecer uma forma simples de rodar LLMs localmente
- **QWen 2.5 Coder** - Modelo state-of-the-art para c√≥digo
- **Claude Code** - Inspira√ß√£o para o design do sistema

---

**üöÄ Comece agora:**
```bash
git clone <repo>
cd ollama-code
make install
ollama-code chat
```
