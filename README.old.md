# ğŸ¤– Ollama Code - AI Code Assistant

> Assistente de cÃ³digo AI inteligente que funciona como Claude Code, 100% local, escrito em Go.

[![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?style=flat&logo=go)](https://go.dev/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Platform](https://img.shields.io/badge/Platform-Linux%20%7C%20Windows%20%7C%20macOS-lightgrey)]()

---

## âœ¨ CaracterÃ­sticas

### Base Features
- ğŸ§  **Linguagem Natural** - Sem comandos especiais (`/read`, `/exec`), apenas fale naturalmente
- ğŸ¯ **DetecÃ§Ã£o Inteligente** - IA detecta automaticamente suas intenÃ§Ãµes usando LLM
- ğŸ”§ **8+ Ferramentas** - Leitura/escrita de arquivos, execuÃ§Ã£o de comandos, git, anÃ¡lise de cÃ³digo
- ğŸ“· **Suporte a Imagens** - LÃª e analisa imagens (PNG, JPG, GIF, etc)
- ğŸŒ **Pesquisa Web** - Busca na internet quando necessÃ¡rio (DuckDuckGo, Stack Overflow, GitHub)
- ğŸ›ï¸ **3 Modos de OperaÃ§Ã£o**:
  - **READ-ONLY**: Somente leitura
  - **INTERACTIVE**: Com confirmaÃ§Ã£o (padrÃ£o)
  - **AUTONOMOUS**: Totalmente automÃ¡tico
- âš¡ **Performance MÃ¡xima** - Startup <15ms, streaming em tempo real
- ğŸ”’ **Privacidade** - 100% local, sem envio de dados para nuvem

### Enterprise Features âœ¨ NEW!
- ğŸ’¾ **Checkpoints & Recovery** - Volte no tempo, desfaÃ§a mudanÃ§as, recupere estados anteriores
- ğŸ“‚ **Session Management** - Salve e retome sessÃµes de trabalho
- ğŸ§  **Hierarchical Memory** - 5 nÃ­veis de memÃ³ria (Enterprise â†’ Project â†’ Rules â†’ User â†’ Local)
- âš¡ **Slash Commands** - 10+ comandos built-in (/help, /checkpoint, /session, /doctor, etc)
- ğŸª **Hooks System** - Pre/post hooks para validaÃ§Ã£o e automaÃ§Ã£o
- ğŸ¨ **Output Styles** - 4 estilos de output (default, explanatory, learning, corporate)
- ğŸš€ **Performance** - Context cache, async tasks, otimizaÃ§Ãµes
- ğŸ¥ **Diagnostics** - /doctor para health checks completos
- ğŸ–¥ï¸ **Hardware Auto-Detection** - Detecta seu hardware e otimiza automaticamente
- âš™ï¸ **3 Presets de ConfiguraÃ§Ã£o** - Compatibility, Performance e Ultra

---

## ğŸ¯ Objetivo

Criar um assistente de cÃ³digo que funciona como Claude Code, mas rodando completamente local usando Ollama.

**Exemplo de uso:**
```bash
$ ollama-code chat

VocÃª: Cria um servidor HTTP em Go com endpoint /health

ğŸ¤–: Vou criar um servidor HTTP bÃ¡sico...

ğŸ”” ConfirmaÃ§Ã£o necessÃ¡ria:
   AÃ§Ã£o: Criar arquivo server.go
   Tipo: WRITE_FILE

Executar? [y/N]: y

âœ… Arquivo criado: server.go

ğŸ¤–: Servidor criado! Quer que eu execute para testar?
```

---

## ğŸ“‹ Requisitos

### Hardware Alvo
- **CPU**: Intel i9 14Âª gen (24 cores) ou similar
- **RAM**: 64GB
- **GPU**: NVIDIA RTX Ada 2000 (16GB VRAM) ou similar
- **Storage**: 1TB NVMe SSD

### Software
- **Go**: 1.21+
- **Ollama**: Ãšltima versÃ£o
- **CUDA**: 11.8+ (para GPU NVIDIA)
- **OS**: Linux, Windows ou macOS

---

## ğŸš€ Como Rodar o Projeto

### PrÃ©-requisitos

1. **Go 1.21+** instalado
2. **Ollama** instalado e rodando
3. **Modelo Ollama** baixado

### InstalaÃ§Ã£o RÃ¡pida (3 passos)

#### 1ï¸âƒ£ Instalar Ollama

```bash
# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Baixe e instale de: https://ollama.ai/download/windows

# macOS
brew install ollama
```

Inicie o serviÃ§o Ollama:
```bash
ollama serve
```

#### 2ï¸âƒ£ Baixar um modelo

```bash
# Modelo recomendado para comeÃ§ar (4.7GB)
ollama pull qwen2.5-coder:7b

# Ou modelos mais poderosos (se tiver GPU com 16GB+ VRAM)
ollama pull qwen2.5-coder:14b-instruct-q5_K_M  # 9.9GB
ollama pull qwen2.5-coder:32b-instruct-q6_K    # 21GB
```

#### 3ï¸âƒ£ Compilar e Executar

```bash
# Clone o repositÃ³rio
git clone https://github.com/JohnPitter/ollama-code.git
cd ollama-code

# Compile a aplicaÃ§Ã£o
./build.sh          # Linux/macOS
# ou
.\build.bat         # Windows

# Execute!
./build/ollama-code chat
```

**Pronto!** A aplicaÃ§Ã£o irÃ¡:
- âœ… Detectar automaticamente seu hardware
- âœ… Criar configuraÃ§Ã£o otimizada em `~/.ollama-code/config.json`
- âœ… Iniciar o modo chat interativo

### Comandos DisponÃ­veis

```bash
# Modo chat interativo (recomendado)
./build/ollama-code chat

# Fazer uma pergunta direta
./build/ollama-code ask "como criar um loop em Go?"

# Modo somente leitura (sem modificaÃ§Ãµes)
./build/ollama-code chat --mode readonly

# Modo autÃ´nomo (sem confirmaÃ§Ãµes)
./build/ollama-code chat --mode autonomous

# Ver ajuda completa
./build/ollama-code help
```

### InstalaÃ§Ã£o Global (Opcional)

Para usar `ollama-code` de qualquer diretÃ³rio:

```bash
# Linux/macOS
sudo cp build/ollama-code /usr/local/bin/
ollama-code chat

# Windows (PowerShell como Admin)
Copy-Item build/ollama-code.exe C:\Windows\System32\
ollama-code chat
```

---

## âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada (Opcional)

### Otimizar para GPU NVIDIA

**Linux/macOS** (`~/.config/ollama/env.conf`):
```bash
export OLLAMA_GPU_LAYERS=999
export OLLAMA_NUM_GPU=1
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_FLASH_ATTENTION=1
export OLLAMA_MAX_VRAM=16384
```

**Windows** (PowerShell como Admin):
```powershell
[System.Environment]::SetEnvironmentVariable('OLLAMA_GPU_LAYERS', '999', 'Machine')
[System.Environment]::SetEnvironmentVariable('OLLAMA_NUM_GPU', '1', 'Machine')
Restart-Service Ollama
```

### Ambiente Corporativo com Proxy

Use os scripts de download direto:
```bash
# Linux/macOS
chmod +x download-models-direct.sh
./download-models-direct.sh

# Windows
.\download-models-direct.ps1
```

### Editar ConfiguraÃ§Ã£o

A aplicaÃ§Ã£o cria automaticamente `~/.ollama-code/config.json` na primeira execuÃ§Ã£o.
Para customizar, edite o arquivo ou veja [CONFIG.md](CONFIG.md)

---

## ğŸ“– Uso

### Primeira ExecuÃ§Ã£o - DetecÃ§Ã£o AutomÃ¡tica de Hardware

Na primeira vez que vocÃª executar o Ollama Code:

```bash
$ ./build/ollama-code chat

ğŸ” First run detected - Analyzing your hardware...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          OLLAMA CODE - HARDWARE DETECTION REPORT           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ–¥ï¸  HARDWARE DETECTED:
   CPU: AMD Ryzen 5 5600 6-Core Processor
   Cores/Threads: 12 / 12
   RAM: 32694 MB total (14494 MB available)
   GPU: NVIDIA GeForce RTX 4070
   VRAM: 12282 MB (1 GPU(s))
   OS: windows / amd64

âš¡ PERFORMANCE TIER: mid-range

ğŸ¯ PRESET SELECTED: performance
   Performance - Balanceamento entre velocidade e compatibilidade

âš™ï¸  OPTIMIZED CONFIGURATION:
   Model: qwen2.5-coder:14b-instruct-q5_K_M
   Temperature: 0.7
   GPU Layers: 35
   Max VRAM: 9825 MB
   Sessions: enabled
   Cache: enabled (15 min)

âœ… Configuration optimized for your hardware!
   Config saved to: ~/.ollama-code/config.json
```

A aplicaÃ§Ã£o automaticamente:
- âœ… Detecta CPU, RAM, GPU e VRAM disponÃ­veis
- âœ… Classifica seu hardware (high-end, mid-range, entry, low-end)
- âœ… Seleciona o melhor preset (ultra, performance, compatibility)
- âœ… Gera configuraÃ§Ã£o otimizada
- âœ… Salva em `~/.ollama-code/config.json`

### Modo Interativo (PadrÃ£o)

```bash
ollama-code chat

VocÃª: Analisa esse projeto
ğŸ¤–: [LÃª arquivos e explica a estrutura]

VocÃª: Cria um servidor REST em Go
ğŸ¤–: [Gera cÃ³digo e pede confirmaÃ§Ã£o antes de criar arquivo]
```

### Modo Read-Only (Somente Leitura)

```bash
ollama-code chat --mode readonly

VocÃª: Mostra o main.go
ğŸ¤–: [Mostra conteÃºdo]

VocÃª: Corrija os erros
âŒ AÃ§Ã£o bloqueada: Escrita nÃ£o permitida em modo READ-ONLY
```

### Modo AutÃ´nomo (Sem ConfirmaÃ§Ã£o)

```bash
ollama-code chat --mode autonomous

VocÃª: Cria um projeto completo com CRUD e testes

[10:23:45] âœ“ Criado: main.go
[10:23:46] âœ“ Criado: handlers/user.go
[10:23:47] âœ“ Criado: tests/user_test.go
[10:23:48] âš™ï¸  go mod tidy
[10:23:49] âš™ï¸  go test ./...
[10:23:52] âœ… Testes passando
```

### Leitura de Imagens

```bash
VocÃª: Leia a imagem screenshot.png e me diga o que tem nela

ğŸ¤–: [LÃª e analisa a imagem]
    A imagem mostra uma interface de usuÃ¡rio com...
```

### Pesquisa na Internet

```bash
VocÃª: Como corrigir erro "permission denied" no Docker?

ğŸŒ Pesquisando na internet...
âœ“ Encontrei 3 fontes relevantes

ğŸ¤–: O erro ocorre quando... [soluÃ§Ã£o com exemplos]

ğŸ“š Fontes:
[1] Stack Overflow - https://...
[2] Docker Docs - https://...
```

---

## ğŸ›ï¸ Modos de OperaÃ§Ã£o

| Modo | Flag | DescriÃ§Ã£o | Uso Recomendado |
|------|------|-----------|-----------------|
| **INTERACTIVE** | `--mode interactive` (padrÃ£o) | Confirma aÃ§Ãµes destrutivas | Desenvolvimento do dia a dia |
| **READ-ONLY** | `--mode readonly` | Apenas leitura | Code review, exploraÃ§Ã£o |
| **AUTONOMOUS** | `--mode autonomous` | Tudo automÃ¡tico | AutomaÃ§Ã£o, prototipagem |

---

## ğŸ”§ Ferramentas DisponÃ­veis

O sistema detecta automaticamente qual ferramenta usar:

- **FileReader**: LÃª arquivos de texto e imagens
- **FileWriter**: Escreve/modifica arquivos
- **CommandExecutor**: Executa comandos shell
- **CodeSearcher**: Busca em cÃ³digo (ripgrep)
- **ProjectAnalyzer**: Analisa estrutura do projeto
- **GitOperations**: OperaÃ§Ãµes git (commit, push, etc)
- **WebSearcher**: Pesquisa na internet

**VocÃª nÃ£o precisa especificar qual ferramenta usar** - a IA escolhe automaticamente baseado no seu pedido!

---

## ğŸ“Š Performance

**No hardware alvo (i9 14Âª gen + RTX Ada 2000):**

```
Startup time:      < 15ms
Memory (base):     ~10MB
Binary size:       ~8MB (otimizado)
LLM throughput:    ~30-40 tokens/s
File operations:   < 10ms
Web search:        ~2-5s (cache: <100ms)
```

---

## ğŸ› ï¸ Desenvolvimento

### Build

```bash
# Build padrÃ£o
make build

# Build otimizado (produÃ§Ã£o)
make build-optimized

# Executar sem instalar
make run

# Testes
make test

# Limpar
make clean
```

### Estrutura do Projeto

```
ollama-code/
â”œâ”€â”€ cmd/ollama-code/main.go          # Entry point
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ agent/                       # Agente principal
â”‚   â”œâ”€â”€ intent/                      # DetecÃ§Ã£o de intenÃ§Ãµes
â”‚   â”œâ”€â”€ tools/                       # Ferramentas
â”‚   â”œâ”€â”€ websearch/                   # Pesquisa web
â”‚   â”œâ”€â”€ llm/                         # Client Ollama
â”‚   â””â”€â”€ confirmation/                # ConfirmaÃ§Ãµes
â”œâ”€â”€ Makefile
â””â”€â”€ IMPLEMENTATION_PLAN.md           # Plano completo
```

---

## ğŸ“š DocumentaÃ§Ã£o

- **[IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md)** - Plano tÃ©cnico completo de implementaÃ§Ã£o (base)
- **[ENTERPRISE_FEATURES.md](ENTERPRISE_FEATURES.md)** - Funcionalidades enterprise-grade completas
- **[download-models-direct.sh](download-models-direct.sh)** - Script para download de modelos (Linux/macOS)
- **[download-models-direct.ps1](download-models-direct.ps1)** - Script para download de modelos (Windows)

---

## ğŸ¤ Contribuindo

Este projeto foi criado como um plano de implementaÃ§Ã£o completo para ser executado por uma IA (como Grok Code Fast 1).

Para contribuir:
1. Leia o [IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md)
2. Siga a estrutura definida
3. Implemente fase por fase
4. Teste cada componente
5. Submeta PR

---

## ğŸ“ Exemplos AvanÃ§ados

### Criar projeto completo
```bash
VocÃª: Cria um projeto REST API em Go com:
      - CRUD de usuÃ¡rios
      - AutenticaÃ§Ã£o JWT
      - Testes unitÃ¡rios
      - Dockerfile
      - README

ğŸ¤–: [Cria estrutura completa do projeto]
```

### AnÃ¡lise e refatoraÃ§Ã£o
```bash
VocÃª: Analisa o cÃ³digo e refatora seguindo Clean Code

ğŸ¤–: [Analisa, sugere melhorias e aplica refatoraÃ§Ãµes]
```

### Debug com pesquisa
```bash
VocÃª: Estou tendo erro X no cÃ³digo, pesquise soluÃ§Ãµes e corrija

ğŸ¤–: ğŸŒ Pesquisando...
    [Encontra soluÃ§Ã£o, aplica correÃ§Ã£o]
```

---

## âš ï¸ Avisos Importantes

1. **Modo AutÃ´nomo**: Use com cuidado! Todas as aÃ§Ãµes sÃ£o executadas sem confirmaÃ§Ã£o.
2. **GPU**: Para melhor performance, configure todas as layers para rodar na GPU.
3. **Proxy Corporativo**: Use os scripts de download direto para baixar modelos.
4. **Backup**: Sempre faÃ§a backup antes de usar modo autÃ´nomo.

---

## ğŸ¯ Roadmap

### Base (10-12 dias)
- [x] DetecÃ§Ã£o inteligente de intenÃ§Ãµes
- [x] 3 modos de operaÃ§Ã£o
- [x] Pesquisa na internet
- [x] Suporte a imagens
- [x] Streaming de respostas
- [x] 8+ ferramentas integradas

### Enterprise (24 dias adicionais)
- [x] **Checkpoints & Rewind** - RecuperaÃ§Ã£o de estado
- [x] **Session Management** - MÃºltiplas sessÃµes, resumir
- [x] **Hierarchical Memory** - CLAUDE.md em 5 nÃ­veis
- [x] **40+ Slash Commands** - CustomizÃ¡veis
- [x] **Hooks System** - Pre/Post execution
- [x] **Telemetry** - OpenTelemetry, mÃ©tricas
- [x] **Sandboxing** - Isolamento de processos
- [x] **/doctor** - Health checks & diagnostics
- [x] **Background Tasks** - Async execution
- [x] **CI/CD** - GitHub Actions, GitLab

### Futuro
- [ ] Cache de embeddings (Redis)
- [ ] Suporte a plugins MCP
- [ ] Interface web
- [ ] IntegraÃ§Ã£o com VS Code

---

## ğŸ“„ LicenÃ§a

Apache 2.0 - Veja [LICENSE](LICENSE)

---

## ğŸ‘¤ Autor

Criado como especificaÃ§Ã£o tÃ©cnica completa para implementaÃ§Ã£o por IA.

**Hardware alvo:** PC high-end com i9 14Âª gen, 64GB RAM, RTX Ada 2000

---

## ğŸ™ Agradecimentos

- **Ollama** - Por fornecer uma forma simples de rodar LLMs localmente
- **QWen 2.5 Coder** - Modelo state-of-the-art para cÃ³digo
- **Claude Code** - InspiraÃ§Ã£o para o design do sistema

---

**ğŸš€ Comece agora:**
```bash
git clone <repo>
cd ollama-code
make install
ollama-code chat
```
