#!/bin/bash

# Setup Otimizado Ollama para RTX Ada 2000 (16GB VRAM)
# Hardware: i9 14th gen, 64GB RAM, RTX Ada 2000

echo "ðŸš€ Configurando Ollama para Performance MÃ¡xima"
echo ""

# VariÃ¡veis de ambiente para Ollama
export OLLAMA_NUM_PARALLEL=4          # Processar 4 requests simultÃ¢neos
export OLLAMA_MAX_LOADED_MODELS=2     # Manter 2 modelos em VRAM
export OLLAMA_FLASH_ATTENTION=1       # Ativar Flash Attention (mais rÃ¡pido)
export OLLAMA_GPU_LAYERS=999          # ForÃ§ar TODAS camadas na GPU
export OLLAMA_HOST=0.0.0.0:11434      # Aceitar conexÃµes de qualquer IP

# CUDA optimizations
export CUDA_VISIBLE_DEVICES=0
export CUDA_LAUNCH_BLOCKING=0

echo "âœ… VariÃ¡veis de ambiente configuradas"
echo ""

# Criar arquivo de configuraÃ§Ã£o persistente
mkdir -p ~/.config/ollama

cat > ~/.config/ollama/env.conf << 'EOF'
# Ollama Performance Configuration
OLLAMA_NUM_PARALLEL=4
OLLAMA_MAX_LOADED_MODELS=2
OLLAMA_FLASH_ATTENTION=1
OLLAMA_GPU_LAYERS=999
OLLAMA_ORIGINS=*
EOF

echo "âœ… Arquivo de configuraÃ§Ã£o criado em ~/.config/ollama/env.conf"
echo ""

# Criar systemd service (se nÃ£o existir)
if [ -d "/etc/systemd/system" ]; then
    echo "ðŸ“ Criando systemd service..."
    sudo tee /etc/systemd/system/ollama.service > /dev/null << 'EOSERVICE'
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
Type=simple
User=$USER
EnvironmentFile=/home/$USER/.config/ollama/env.conf
ExecStart=/usr/local/bin/ollama serve
Restart=always
RestartSec=3

[Install]
WantedBy=default.target
EOSERVICE

    sudo systemctl daemon-reload
    sudo systemctl enable ollama
    sudo systemctl restart ollama
    
    echo "âœ… Systemd service configurado e iniciado"
else
    echo "âš ï¸  Systemd nÃ£o disponÃ­vel, inicie manualmente com:"
    echo "   source ~/.config/ollama/env.conf && ollama serve"
fi

echo ""
echo "ðŸŽ¯ Modelos Recomendados para seu Hardware:"
echo ""
echo "   Para CÃ“DIGO (principal uso):"
echo "   â”œâ”€ deepseek-coder-v2:16b-lite-instruct-q8_0  # RÃ¡pido, 16GB total"
echo "   â”œâ”€ qwen2.5-coder:32b-instruct-q6_K           # Melhor qualidade"
echo "   â””â”€ codestral:22b-v0.1-q8_0                   # Balanceado"
echo ""
echo "   Para CHAT/ExplicaÃ§Ãµes:"
echo "   â”œâ”€ qwen2.5:32b-instruct-q6_K                 # Excelente reasoning"
echo "   â””â”€ llama3.1:70b-instruct-q4_K_M              # MÃ¡xima capacidade"
echo ""
echo "   Para EMBEDDINGS (contexto):"
echo "   â””â”€ nomic-embed-text                          # Leve e eficiente"
echo ""

echo "ðŸ’¡ Comandos para instalar:"
echo ""
echo "# Modelo principal (CODING) - Recomendado"
echo "ollama pull qwen2.5-coder:32b-instruct-q6_K"
echo ""
echo "# Modelo secundÃ¡rio (CHAT)"
echo "ollama pull qwen2.5:32b-instruct-q6_K"
echo ""
echo "# Embeddings"
echo "ollama pull nomic-embed-text"
echo ""

echo "ðŸ”§ Verificar se GPU estÃ¡ sendo usada:"
echo "   nvidia-smi -l 1"
echo ""
echo "ðŸ“Š Testar performance:"
echo "   ollama run qwen2.5-coder:32b-instruct-q6_K 'Write a fibonacci function in Python'"
echo ""

echo "âœ… Setup completo!"
